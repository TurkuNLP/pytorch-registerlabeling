data:
  train: fi
model:
  name: facebook/xlm-roberta-xl
trainer:
  learning_rate: 1e-4
  gradient_accumulation_steps: 8
  best_model_metric: eval_f1
peft:
  enable: True
dataloader:
  train_batch_size: 1
  test_batch_size: 64
  train_batch_size: 64
# 42 {'f1': 0.794572591587517, 'f1_th05': 0.7873689659635046, 'precision': 0.7943570265870863, 'recall': 0.7947882736156352, 'pr_auc': 0.864586000447274, 'roc_auc': 0.8898279073454392, 'accuracy': 0.6899116689911668, 'threshold': 0.5999999999999999}
# 43 {'f1': 0.791522371519601, 'f1_th05': 0.782564493582412, 'precision': 0.8082036775106082, 'recall': 0.7755157437567861, 'pr_auc': 0.8745635488205998, 'roc_auc': 0.8809901890611205, 'accuracy': 0.6838679683867969, 'threshold': 0.5999999999999999}
# Note: the above was with batch 1 and accumulation 8