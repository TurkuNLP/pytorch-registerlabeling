{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = \".hf/transformers_cache\"\n",
    "os.environ[\"HF_HOME\"] = \".hf/hf_home\"\n",
    "os.environ[\"XDG_CACHE_HOME\"] = \".hf/xdg_cache_home\"\n",
    "import shlex\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from argparse import ArgumentParser\n",
    "import re\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "from ray.tune import grid_search, CLIReporter\n",
    "import numpy as np\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    Trainer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    EarlyStoppingCallback,\n",
    ")\n",
    "from datasets import load_dataset, Features, Value\n",
    "from torch.nn import BCEWithLogitsLoss, Sigmoid\n",
    "from torch import Tensor, FloatTensor, bfloat16\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    ")\n",
    "\n",
    "from peft import LoraConfig, get_peft_model, TaskType, prepare_model_for_int8_training\n",
    "\n",
    "import wandb\n",
    "\n",
    "args = shlex.split(\"--train fi --test fi --evaluate_only \")\n",
    "\n",
    "parser = ArgumentParser(args)\n",
    "parser.add_argument(\"--model_name\", type=str, default=\"xlm-roberta-base\")\n",
    "parser.add_argument(\"--custom_tokenizer\", type=str, default=None)\n",
    "parser.add_argument(\"--train\", type=str, required=True)\n",
    "parser.add_argument(\"--test\", type=str, required=True)\n",
    "parser.add_argument(\"--lr\", type=float, default=1e-5)\n",
    "parser.add_argument(\"--train_batch_size\", type=int, default=8)\n",
    "parser.add_argument(\"--eval_batch_size\", type=int, default=8)\n",
    "parser.add_argument(\"--num_epochs\", type=int, default=15)\n",
    "parser.add_argument(\"--weight_decay\", type=float, default=0)\n",
    "parser.add_argument(\"--patience\", type=int, default=5)\n",
    "parser.add_argument(\"--gradient_steps\", type=int, default=1)\n",
    "parser.add_argument(\"--epochs\", type=int, default=20)\n",
    "parser.add_argument(\"--iter_strategy\", type=str, default=\"epoch\")\n",
    "parser.add_argument(\"--eval_steps\", type=int, default=100)\n",
    "parser.add_argument(\"--logging_steps\", type=int, default=100)\n",
    "parser.add_argument(\"--save_steps\", type=int, default=100)\n",
    "parser.add_argument(\"--save_model\", action=\"store_true\", default=True)\n",
    "parser.add_argument(\"--overwrite\", action=\"store_true\")\n",
    "parser.add_argument(\"--report_to\", type=str, default=None)\n",
    "\n",
    "parser.add_argument(\n",
    "    \"--data_path\",\n",
    "    type=str,\n",
    "    default=\"data\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--output_path\",\n",
    "    type=str,\n",
    "    default=\"/scratch/project_2005092/register-models\",\n",
    ")\n",
    "\n",
    "parser.add_argument(\n",
    "    \"--max_length\",\n",
    "    type=int,\n",
    "    default=512,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--set_pad_id\",\n",
    "    action=\"store_true\",\n",
    "    help=\"Set the id for the padding token, needed by models such as Mistral-7B\",\n",
    ")\n",
    "\n",
    "parser.add_argument(\n",
    "    \"--hp_search\",\n",
    "    action=\"store_true\",\n",
    "    help=\"Hyperparameter search\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--evaluate_only\",\n",
    "    action=\"store_true\",\n",
    "    help=\"Only evaluate the model using test set\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--quantize\",\n",
    "    action=\"store_true\",\n",
    "    help=\"Use quantization\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--peft\",\n",
    "    action=\"store_true\",\n",
    "    help=\"Do Parameter Efficient Fine-Tuning\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--class_weights\",\n",
    "    action=\"store_true\",\n",
    "    help=\"Use class weights\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--threshold\",\n",
    "    type=float,\n",
    "    default=None,\n",
    ")\n",
    "parser.add_argument(\"--lora_rank\", type=int, default=16)\n",
    "parser.add_argument(\"--lora_alpha\", type=float, default=8)\n",
    "parser.add_argument(\n",
    "    \"--lora_dropout\",\n",
    "    type=float,\n",
    "    default=0.05,\n",
    ")\n",
    "parser.add_argument(\"--lora_bias\", type=str, default=\"none\")\n",
    "parser.add_argument(\"--seed\", type=str, default=42)\n",
    "parser.add_argument(\"--labels\", choices=[\"full\", \"upper\"], default=\"full\")\n",
    "\n",
    "options = parser.parse_args()\n",
    "\n",
    "print(f\"Settings: {options}\")\n",
    "\n",
    "\n",
    "# Register config\n",
    "\n",
    "\n",
    "labels_full = [\n",
    "    \"HI\",\n",
    "    \"ID\",\n",
    "    \"IN\",\n",
    "    \"IP\",\n",
    "    \"LY\",\n",
    "    \"MT\",\n",
    "    \"NA\",\n",
    "    \"OP\",\n",
    "    \"SP\",\n",
    "    \"av\",\n",
    "    \"ds\",\n",
    "    \"dtp\",\n",
    "    \"ed\",\n",
    "    \"en\",\n",
    "    \"fi\",\n",
    "    \"it\",\n",
    "    \"lt\",\n",
    "    \"nb\",\n",
    "    \"ne\",\n",
    "    \"ob\",\n",
    "    \"ra\",\n",
    "    \"re\",\n",
    "    \"rs\",\n",
    "    \"rv\",\n",
    "    \"sr\",\n",
    "]\n",
    "\n",
    "labels_upper = [\"HI\", \"ID\", \"IN\", \"IP\", \"LY\", \"MT\", \"NA\", \"OP\", \"SP\"]\n",
    "\n",
    "sub_register_map = {\n",
    "    \"NA\": \"NA\",\n",
    "    \"NE\": \"ne\",\n",
    "    \"SR\": \"sr\",\n",
    "    \"PB\": \"nb\",\n",
    "    \"HA\": \"NA\",\n",
    "    \"FC\": \"NA\",\n",
    "    \"TB\": \"nb\",\n",
    "    \"CB\": \"nb\",\n",
    "    \"OA\": \"NA\",\n",
    "    \"OP\": \"OP\",\n",
    "    \"OB\": \"ob\",\n",
    "    \"RV\": \"rv\",\n",
    "    \"RS\": \"rs\",\n",
    "    \"AV\": \"av\",\n",
    "    \"IN\": \"IN\",\n",
    "    \"JD\": \"IN\",\n",
    "    \"FA\": \"fi\",\n",
    "    \"DT\": \"dtp\",\n",
    "    \"IB\": \"IN\",\n",
    "    \"DP\": \"dtp\",\n",
    "    \"RA\": \"ra\",\n",
    "    \"LT\": \"lt\",\n",
    "    \"CM\": \"IN\",\n",
    "    \"EN\": \"en\",\n",
    "    \"RP\": \"IN\",\n",
    "    \"ID\": \"ID\",\n",
    "    \"DF\": \"ID\",\n",
    "    \"QA\": \"ID\",\n",
    "    \"HI\": \"HI\",\n",
    "    \"RE\": \"re\",\n",
    "    \"IP\": \"IP\",\n",
    "    \"DS\": \"ds\",\n",
    "    \"EB\": \"ed\",\n",
    "    \"ED\": \"ed\",\n",
    "    \"LY\": \"LY\",\n",
    "    \"PO\": \"LY\",\n",
    "    \"SO\": \"LY\",\n",
    "    \"SP\": \"SP\",\n",
    "    \"IT\": \"it\",\n",
    "    \"FS\": \"SP\",\n",
    "    \"TV\": \"SP\",\n",
    "    \"OS\": \"OS\",\n",
    "    \"IG\": \"IP\",\n",
    "    \"MT\": \"MT\",\n",
    "    \"HT\": \"HI\",\n",
    "    \"FI\": \"fi\",\n",
    "    \"OI\": \"IN\",\n",
    "    \"TR\": \"IN\",\n",
    "    \"AD\": \"OP\",\n",
    "    \"LE\": \"OP\",\n",
    "    \"OO\": \"OP\",\n",
    "    \"MA\": \"NA\",\n",
    "    \"ON\": \"NA\",\n",
    "    \"SS\": \"NA\",\n",
    "    \"OE\": \"IP\",\n",
    "    \"PA\": \"IP\",\n",
    "    \"OF\": \"ID\",\n",
    "    \"RR\": \"ID\",\n",
    "    \"FH\": \"HI\",\n",
    "    \"OH\": \"HI\",\n",
    "    \"TS\": \"HI\",\n",
    "    \"OL\": \"LY\",\n",
    "    \"PR\": \"LY\",\n",
    "    \"SL\": \"LY\",\n",
    "    \"TA\": \"SP\",\n",
    "    \"OTHER\": \"OS\",\n",
    "    \"\": \"\",\n",
    "}\n",
    "\n",
    "small_languages = [\n",
    "    \"ar\",\n",
    "    \"ca\",\n",
    "    \"es\",\n",
    "    \"fa\",\n",
    "    \"hi\",\n",
    "    \"id\",\n",
    "    \"jp\",\n",
    "    \"no\",\n",
    "    \"pt\",\n",
    "    \"tr\",\n",
    "    \"ur\",\n",
    "    \"zh\",\n",
    "]\n",
    "\n",
    "\n",
    "# Data column structures\n",
    "\n",
    "cols = {\n",
    "    \"fr\": [\"a\", \"b\", \"label\", \"text\", \"c\"],\n",
    "    \"fi\": [\"label\", \"text\", \"a\", \"b\", \"c\"],\n",
    "    \"sv\": [\"a\", \"b\", \"label\", \"text\", \"c\"],\n",
    "}\n",
    "\n",
    "# Common variables\n",
    "\n",
    "labels = labels_full if options.labels == \"full\" else labels_upper\n",
    "model_name = options.model_name\n",
    "working_dir = f\"{options.output_path}/{options.train}_{options.test}{'_tuning' if options.hp_search else ''}/{model_name.replace('/', '_')}\"\n",
    "\n",
    "\n",
    "# Wandb setup\n",
    "\n",
    "if options.report_to == \"wandb\":\n",
    "    os.environ[\n",
    "        \"WANDB_PROJECT\"\n",
    "    ] = f\"register-labeling_{options.train}_{options.test}{'_tuning' if options.hp_search else ''}_{model_name.replace('/', '_')}\"\n",
    "\n",
    "    load_dotenv()\n",
    "    os.environ[\"WANDB_API_KEY\"] = os.getenv(\"WANDB_API_KEY\")\n",
    "    wandb.login()\n",
    "\n",
    "\n",
    "# Data preprocessing\n",
    "\n",
    "\n",
    "def preprocess_data(example):\n",
    "    text = example[\"text\"] or \"\"\n",
    "    encoding = tokenizer(\n",
    "        text, padding=\"max_length\", truncation=True, max_length=options.max_length\n",
    "    )\n",
    "    mapped_labels = set(\n",
    "        [\n",
    "            sub_register_map[l] if l not in labels else l\n",
    "            for l in (example[\"label\"] or \"NA\").split()\n",
    "        ]\n",
    "    )\n",
    "    encoding[\"label\"] = [1 if l in mapped_labels else 0 for l in labels]\n",
    "    return encoding\n",
    "\n",
    "\n",
    "def get_data():\n",
    "    data_files = {\"train\": [], \"dev\": [], \"test\": []}\n",
    "\n",
    "    for l in options.train.split(\"-\"):\n",
    "        data_files[\"train\"].append(f\"data/{l}/train.tsv\")\n",
    "        if not (l in small_languages):\n",
    "            data_files[\"dev\"].append(f\"data/{l}/dev.tsv\")\n",
    "        else:\n",
    "            # Small languages use test as dev\n",
    "            data_files[\"dev\"].append(f\"data/{l}/test.tsv\")\n",
    "\n",
    "    for l in options.test.split(\"-\"):\n",
    "        # check if zero-shot for small languages, if yes then test with full data\n",
    "        if l in small_languages and not (l in options.train.split(\"-\")):\n",
    "            data_files[\"test\"].append(f\"data/{l}/{l}.tsv\")\n",
    "        else:\n",
    "            data_files[\"test\"].append(f\"data/{l}/test.tsv\")\n",
    "\n",
    "    return data_files\n",
    "\n",
    "\n",
    "print(\"Getting data...\")\n",
    "\n",
    "data_files = get_data()\n",
    "\n",
    "print(\"data files\", data_files)\n",
    "\n",
    "dataset = load_dataset(\n",
    "    \"csv\",\n",
    "    data_files=data_files,\n",
    "    delimiter=\"\\t\",\n",
    "    column_names=cols.get(options.train, [\"label\", \"text\"]),\n",
    "    features=Features(\n",
    "        {\n",
    "            \"text\": Value(\"string\"),\n",
    "            \"label\": Value(\"string\"),\n",
    "        }\n",
    "    ),\n",
    "    cache_dir=f\"{working_dir}/dataset_cache\",\n",
    ")\n",
    "dataset = dataset.shuffle(seed=options.seed)\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name if not options.custom_tokenizer else options.custom_tokenizer\n",
    ")\n",
    "\n",
    "if options.set_pad_id:\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(\"Preprocessing...\")\n",
    "\n",
    "dataset = dataset.map(preprocess_data)\n",
    "\n",
    "print(\"Got preprocessed dataset and tokenizer\")\n",
    "\n",
    "\n",
    "# Start modeling\n",
    "\n",
    "if options.class_weights:\n",
    "    y = [\n",
    "        i\n",
    "        for example in dataset[\"train\"]\n",
    "        for i, val in enumerate(example[\"label\"])\n",
    "        if val\n",
    "    ]\n",
    "\n",
    "    weights = len(dataset[\"train\"]) / (len(labels) * np.bincount(y))\n",
    "    class_weights = FloatTensor(weights)\n",
    "\n",
    "    print(f\"using class weights: {class_weights}\")\n",
    "\n",
    "\n",
    "class MultilabelTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        if options.class_weights:\n",
    "            loss_fct = BCEWithLogitsLoss(pos_weight=class_weights)\n",
    "        else:\n",
    "            loss_fct = BCEWithLogitsLoss()\n",
    "        loss = loss_fct(\n",
    "            logits.view(-1, self.model.config.num_labels),\n",
    "            labels.float().view(-1, self.model.config.num_labels),\n",
    "        )\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "\n",
    "def optimize_threshold(predictions, labels):\n",
    "    sigmoid = Sigmoid()\n",
    "    probs = sigmoid(Tensor(predictions))\n",
    "    best_f1 = 0\n",
    "    best_f1_threshold = 0.5\n",
    "    y_true = labels\n",
    "    for th in np.arange(0.3, 0.7, 0.05):\n",
    "        y_pred = np.zeros(probs.shape)\n",
    "        y_pred[np.where(probs >= th)] = 1\n",
    "        f1 = f1_score(y_true=y_true, y_pred=y_pred, average=\"micro\")\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_f1_threshold = th\n",
    "    return best_f1_threshold\n",
    "\n",
    "\n",
    "def compute_metrics(p):\n",
    "    _, labels = p\n",
    "    predictions = (\n",
    "        p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions\n",
    "    )\n",
    "    threshold = (\n",
    "        options.threshold\n",
    "        if options.threshold\n",
    "        else optimize_threshold(predictions, labels)\n",
    "    )\n",
    "    sigmoid = Sigmoid()\n",
    "    probs = sigmoid(Tensor(predictions))\n",
    "    y_pred = np.zeros(probs.shape)\n",
    "    y_pred[np.where(probs >= threshold)] = 1\n",
    "    y_th05 = np.zeros(probs.shape)\n",
    "    y_th05[np.where(probs >= 0.5)] = 1\n",
    "    roc_auc = roc_auc_score(labels, y_pred, average=\"micro\")\n",
    "    accuracy = accuracy_score(labels, y_pred)\n",
    "    metrics = {\n",
    "        \"f1\": f1_score(y_true=labels, y_pred=y_pred, average=\"micro\"),\n",
    "        \"f1_th05\": f1_score(y_true=labels, y_pred=y_th05, average=\"micro\"),\n",
    "        \"roc_auc\": roc_auc,\n",
    "        \"accuracy\": accuracy,\n",
    "        \"threshold\": threshold,\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def model_init():\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_name,\n",
    "        num_labels=len(labels),\n",
    "        cache_dir=f\"{working_dir}/model_cache\",\n",
    "        trust_remote_code=True,\n",
    "        device_map=\"auto\",\n",
    "        low_cpu_mem_usage=True,\n",
    "        quantization_config=BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_compute_dtype=bfloat16,\n",
    "        )\n",
    "        if options.quantize\n",
    "        else None,\n",
    "    )\n",
    "\n",
    "    if options.peft:\n",
    "        # Get module names\n",
    "\n",
    "        model_modules = str(model.modules)\n",
    "        pattern = r\"\\((\\w+)\\): Linear\"\n",
    "        linear_layer_names = re.findall(pattern, model_modules)\n",
    "\n",
    "        names = []\n",
    "        # Print the names of the Linear layers\n",
    "        for name in linear_layer_names:\n",
    "            names.append(name)\n",
    "        target_modules = list(set(names))\n",
    "\n",
    "        # Define LoRA Config\n",
    "        lora_config = LoraConfig(\n",
    "            r=options.lora_rank,\n",
    "            lora_alpha=options.lora_alpha,\n",
    "            target_modules=target_modules,\n",
    "            lora_dropout=options.lora_dropout,\n",
    "            bias=options.lora_bias,\n",
    "            task_type=TaskType.SEQ_CLS,\n",
    "            inference_mode=True,\n",
    "        )\n",
    "\n",
    "        # add LoRA adaptor\n",
    "        model.gradient_checkpointing_enable()\n",
    "        model.config.pad_token_id = model.config.eos_token_id\n",
    "        model.config.use_cache = False\n",
    "        model = prepare_model_for_int8_training(model)\n",
    "        model = get_peft_model(model, lora_config)\n",
    "        model.print_trainable_parameters()\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "trainer = MultilabelTrainer(\n",
    "    model=None,\n",
    "    model_init=model_init,\n",
    "    args=TrainingArguments(\n",
    "        f\"{working_dir}/checkpoints\",\n",
    "        overwrite_output_dir=True if options.overwrite else False,\n",
    "        evaluation_strategy=options.iter_strategy,\n",
    "        save_strategy=options.iter_strategy,\n",
    "        logging_strategy=options.iter_strategy,\n",
    "        load_best_model_at_end=True,\n",
    "        eval_steps=options.eval_steps,\n",
    "        logging_steps=options.logging_steps,\n",
    "        save_steps=options.save_steps,\n",
    "        learning_rate=options.lr,\n",
    "        metric_for_best_model=\"eval_f1\",\n",
    "        greater_is_better=True,\n",
    "        per_device_train_batch_size=options.train_batch_size,\n",
    "        per_device_eval_batch_size=options.eval_batch_size,\n",
    "        num_train_epochs=options.epochs,\n",
    "        gradient_checkpointing=True,\n",
    "        gradient_accumulation_steps=options.gradient_steps,\n",
    "        report_to=options.report_to,\n",
    "    ),\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"dev\"],\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=options.patience)],\n",
    ")\n",
    "\n",
    "if not options.evaluate_only:\n",
    "    if not options.hp_search:\n",
    "        print(\"Training...\")\n",
    "        trainer.train()\n",
    "\n",
    "    else:\n",
    "        asha_scheduler = ASHAScheduler(\n",
    "            metric=\"eval_f1\",\n",
    "            mode=\"max\",\n",
    "        )\n",
    "\n",
    "        tune_config = {\n",
    "            \"learning_rate\": grid_search([1e-6, 5e-6, 1e-5, 5e-5, 1e-4, 5e-4]),\n",
    "            \"per_device_train_batch_size\": grid_search([6, 8, 12]),\n",
    "        }\n",
    "\n",
    "        reporter = CLIReporter(\n",
    "            parameter_columns={\n",
    "                \"learning_rate\": \"learning_rate\",\n",
    "                \"per_device_train_batch_size\": \"train_bs/gpu\",\n",
    "                \"num_train_epochs\": \"num_epochs\",\n",
    "            },\n",
    "            metric_columns=[\n",
    "                \"eval_f1\",\n",
    "                \"eval_f1_th05\",\n",
    "                \"eval_threshold\",\n",
    "                \"training_iteration\",\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        best_model = trainer.hyperparameter_search(\n",
    "            hp_space=lambda _: tune_config,\n",
    "            backend=\"ray\",\n",
    "            scheduler=asha_scheduler,\n",
    "            progress_reporter=reporter,\n",
    "            direction=\"maximize\",\n",
    "            local_dir=f\"{working_dir}/ray\",\n",
    "            log_to_file=True,\n",
    "        )\n",
    "        print(\"Best model according to Ray:\")\n",
    "        print(best_model)\n",
    "\n",
    "print(\"Evaluating with dev set...\")\n",
    "print(trainer.evaluate(dataset[\"dev\"]))\n",
    "\n",
    "print(\"Evaluating with test set...\")\n",
    "print(trainer.evaluate(dataset[\"test\"]))\n",
    "\n",
    "test_pred = trainer.predict(dataset[\"test\"])\n",
    "trues = test_pred.label_ids\n",
    "predictions = test_pred.predictions\n",
    "threshold = (\n",
    "    options.threshold if options.threshold else optimize_threshold(predictions, trues)\n",
    ")\n",
    "sigmoid = Sigmoid()\n",
    "probs = sigmoid(Tensor(predictions))\n",
    "preds = np.zeros(probs.shape)\n",
    "preds[np.where(probs >= threshold)] = 1\n",
    "\n",
    "print(classification_report(trues, preds, target_names=labels))\n",
    "\n",
    "if not options.evaluate_only and options.save_model:\n",
    "    trainer.model.save_pretrained(f\"{working_dir}/saved_model\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
